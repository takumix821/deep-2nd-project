{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5401a171",
   "metadata": {},
   "source": [
    "### Confirm Tensorflow Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "304bcb56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n",
      "2.6.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "print(tf.__version__)\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e3c573b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]='0'\n",
    "#tf.test.is_built_with_cuda()\n",
    "#tf.test.is_built_with_gpu_support()\n",
    "#tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9702e06",
   "metadata": {},
   "source": [
    "### Importing the necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9c932c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from os import listdir \n",
    "from os.path import isfile, join\n",
    "import cv2\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02686e89",
   "metadata": {},
   "source": [
    "### Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e0278dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 25000 files.\n",
      "folder: ['0', '1', '2', '3', '4']\n"
     ]
    }
   ],
   "source": [
    "#Data visiting â€“ os.walk()\n",
    "\n",
    "label_folder = []\n",
    "total_size = 0\n",
    "data_path = r\"D:\\CIFAR10_Test Image\\Training_data\"\n",
    "\n",
    "#os.walk() generates the file names(dirpath, dirnames, filenames) \n",
    "#in a directory tree by walking the tree either top-down or bottom-up.\n",
    "for root, dirts, files in os.walk(data_path): \n",
    "    for dirt in dirts:\n",
    "        label_folder.append(dirt)\n",
    "    total_size += len(files)\n",
    "\n",
    "    \n",
    "print(\"found\",total_size,\"files.\")\n",
    "print(\"folder:\",label_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ecec1475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 32, 32, 3)\n",
      "(25000,)\n"
     ]
    }
   ],
   "source": [
    "#Load image\n",
    "\n",
    "base_x_train = []\n",
    "base_y_train = []\n",
    "\n",
    "for i in range(len(label_folder)):\n",
    "    labelPath = data_path+r'\\\\'+label_folder[i]\n",
    "    \n",
    "    #listdir() returns a list containing the names of the entries in the directory given by path.\n",
    "    #isfile() is used to check whether the specified path is an existing regular file or not.\n",
    "    FileName = [f for f in listdir(labelPath) if isfile(join(labelPath, f))]\n",
    "    \n",
    "    for j in range(len(FileName)):\n",
    "        path = labelPath+r'\\\\'+FileName[j]\n",
    "        \n",
    "        #use cv2.imread read image.\n",
    "        img = cv2.imread(path,cv2.IMREAD_COLOR)\n",
    "        \n",
    "        base_x_train.append(img)\n",
    "        base_y_train.append(label_folder[i])\n",
    "\n",
    "\n",
    "print(np.array(base_x_train).shape)\n",
    "print(np.array(base_y_train).shape)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4524f916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 32, 32, 3)\n",
      "(25000, 5)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "#Convert a category vector to a binary (0 or 1) matrix-type representation\n",
    "\n",
    "base_y_train = to_categorical(base_y_train)\n",
    "\n",
    "\n",
    "print(np.array(base_x_train).shape)\n",
    "print(np.array(base_y_train).shape)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbef7560",
   "metadata": {},
   "source": [
    "### Splitting the Data into training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8b2f1af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: (20000, 32, 32, 3) (20000, 5)\n",
      "Validation data: (5000, 32, 32, 3) (5000, 5)\n"
     ]
    }
   ],
   "source": [
    "# Split into training and validation sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_valid, y_train, y_valid = train_test_split( \\\n",
    "    np.array(base_x_train), np.array(base_y_train), test_size=0.2, random_state = 0)\n",
    "\n",
    "print(\"Training data:\", x_train.shape, y_train.shape)\n",
    "print(\"Validation data:\", x_valid.shape, y_valid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845fcdd8",
   "metadata": {},
   "source": [
    "### Show the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cee274df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 1\n",
      "Answer(one-hot): [0. 1. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "idx = random.randint(0, x_train.shape[0])\n",
    "plt.imshow(x_train[idx])\n",
    "plt.show()\n",
    "\n",
    "print(\"Answer:\", np.argmax(y_train[idx]))\n",
    "print(\"Answer(one-hot):\", y_train[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03800027",
   "metadata": {},
   "source": [
    "### Build the network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77060e81",
   "metadata": {},
   "source": [
    "### Training the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47cd5da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), padding=\"same\", activation='relu', input_shape=(32, 32, 3)),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), padding=\"same\", activation='relu'),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    tf.keras.layers.Conv2D(128, (3, 3), padding=\"same\", activation='relu'),\n",
    "    tf.keras.layers.Conv2D(128, (3, 3), padding=\"same\", activation='relu'),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    tf.keras.layers.Conv2D(256, (4, 4), padding=\"same\", activation='relu'),\n",
    "    tf.keras.layers.Conv2D(256, (4, 4), padding=\"same\", activation='relu'),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation='sigmoid'),\n",
    "    tf.keras.layers.Dense(128, activation='sigmoid'),\n",
    "    tf.keras.layers.Dense(5, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59632ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "400/400 [==============================] - 65s 129ms/step - loss: 1.4973 - accuracy: 0.3711 - val_loss: 1.3535 - val_accuracy: 0.4920\n",
      "Epoch 2/15\n",
      "400/400 [==============================] - 52s 130ms/step - loss: 1.2362 - accuracy: 0.5200 - val_loss: 1.1276 - val_accuracy: 0.5734\n",
      "Epoch 3/15\n",
      "400/400 [==============================] - 52s 130ms/step - loss: 1.0630 - accuracy: 0.5881 - val_loss: 0.9983 - val_accuracy: 0.6036\n",
      "Epoch 4/15\n",
      "400/400 [==============================] - 52s 130ms/step - loss: 0.9399 - accuracy: 0.6392 - val_loss: 0.9069 - val_accuracy: 0.6566\n",
      "Epoch 5/15\n",
      "400/400 [==============================] - 52s 131ms/step - loss: 0.8421 - accuracy: 0.6844 - val_loss: 0.8203 - val_accuracy: 0.6890\n",
      "Epoch 6/15\n",
      "400/400 [==============================] - 53s 132ms/step - loss: 0.7688 - accuracy: 0.7106 - val_loss: 0.7714 - val_accuracy: 0.7056\n",
      "Epoch 7/15\n",
      "400/400 [==============================] - 53s 131ms/step - loss: 0.6908 - accuracy: 0.7419 - val_loss: 0.7399 - val_accuracy: 0.7212\n",
      "Epoch 8/15\n",
      "400/400 [==============================] - 52s 129ms/step - loss: 0.6270 - accuracy: 0.7712 - val_loss: 0.6695 - val_accuracy: 0.7518\n",
      "Epoch 9/15\n",
      "400/400 [==============================] - 51s 127ms/step - loss: 0.5671 - accuracy: 0.7950 - val_loss: 0.6463 - val_accuracy: 0.7602\n",
      "Epoch 10/15\n",
      "400/400 [==============================] - 51s 129ms/step - loss: 0.5036 - accuracy: 0.8221 - val_loss: 0.6178 - val_accuracy: 0.7688\n",
      "Epoch 11/15\n",
      "400/400 [==============================] - 51s 127ms/step - loss: 0.4396 - accuracy: 0.8467 - val_loss: 0.5728 - val_accuracy: 0.7918\n",
      "Epoch 12/15\n",
      "400/400 [==============================] - 51s 127ms/step - loss: 0.3860 - accuracy: 0.8679 - val_loss: 0.6077 - val_accuracy: 0.7752\n",
      "Epoch 13/15\n",
      "400/400 [==============================] - 51s 127ms/step - loss: 0.3219 - accuracy: 0.8957 - val_loss: 0.5830 - val_accuracy: 0.7870\n",
      "Epoch 14/15\n",
      "400/400 [==============================] - 51s 126ms/step - loss: 0.2703 - accuracy: 0.9169 - val_loss: 0.5221 - val_accuracy: 0.8136\n",
      "Epoch 15/15\n",
      "400/400 [==============================] - 50s 125ms/step - loss: 0.2109 - accuracy: 0.9398 - val_loss: 0.5169 - val_accuracy: 0.8120\n"
     ]
    }
   ],
   "source": [
    "epoch = 15\n",
    "batch_size = 50\n",
    "\n",
    "model.compile(\n",
    "    loss=\"categorical_crossentropy\", \n",
    "    optimizer=\"sgd\", \n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "history = model.fit(\n",
    "    x_train, \n",
    "    y_train, \n",
    "    epochs=epoch, \n",
    "    batch_size=batch_size,\n",
    "    validation_data=(x_valid, y_valid)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47cd5da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), padding=\"same\", activation='relu', input_shape=(32, 32, 3)),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), padding=\"same\", activation='relu'),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    tf.keras.layers.Conv2D(128, (3, 3), padding=\"same\", activation='relu'),\n",
    "    tf.keras.layers.Conv2D(128, (3, 3), padding=\"same\", activation='relu'),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    tf.keras.layers.Conv2D(256, (4, 4), padding=\"same\", activation='relu'),\n",
    "    tf.keras.layers.Conv2D(256, (4, 4), padding=\"same\", activation='relu'),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation='sigmoid'),\n",
    "    tf.keras.layers.Dense(128, activation='sigmoid'),\n",
    "    tf.keras.layers.Dense(5, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59632ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "400/400 [==============================] - 51s 122ms/step - loss: 1.6198 - accuracy: 0.2017 - val_loss: 1.6265 - val_accuracy: 0.2036\n",
      "Epoch 2/15\n",
      "400/400 [==============================] - 49s 122ms/step - loss: 1.6168 - accuracy: 0.2034 - val_loss: 1.6268 - val_accuracy: 0.2040\n",
      "Epoch 3/15\n",
      "400/400 [==============================] - 51s 127ms/step - loss: 1.6171 - accuracy: 0.2016 - val_loss: 1.6246 - val_accuracy: 0.2028\n",
      "Epoch 4/15\n",
      "400/400 [==============================] - 48s 119ms/step - loss: 1.6148 - accuracy: 0.1991 - val_loss: 1.6238 - val_accuracy: 0.1934\n",
      "Epoch 5/15\n",
      "400/400 [==============================] - 48s 121ms/step - loss: 1.6137 - accuracy: 0.1991 - val_loss: 1.6098 - val_accuracy: 0.2040\n",
      "Epoch 6/15\n",
      "400/400 [==============================] - 48s 120ms/step - loss: 1.6139 - accuracy: 0.1992 - val_loss: 1.6124 - val_accuracy: 0.2028\n",
      "Epoch 7/15\n",
      "400/400 [==============================] - 49s 122ms/step - loss: 1.6125 - accuracy: 0.2035 - val_loss: 1.6096 - val_accuracy: 0.2028\n",
      "Epoch 8/15\n",
      "400/400 [==============================] - 49s 122ms/step - loss: 1.6120 - accuracy: 0.1989 - val_loss: 1.6116 - val_accuracy: 0.2040\n",
      "Epoch 9/15\n",
      "400/400 [==============================] - 49s 123ms/step - loss: 1.6121 - accuracy: 0.1996 - val_loss: 1.6107 - val_accuracy: 0.1962\n",
      "Epoch 10/15\n",
      "400/400 [==============================] - 48s 120ms/step - loss: 1.6118 - accuracy: 0.1993 - val_loss: 1.6111 - val_accuracy: 0.2028\n",
      "Epoch 11/15\n",
      "400/400 [==============================] - 47s 118ms/step - loss: 1.6115 - accuracy: 0.1989 - val_loss: 1.6129 - val_accuracy: 0.1962\n",
      "Epoch 12/15\n",
      "400/400 [==============================] - 46s 115ms/step - loss: 1.6114 - accuracy: 0.2017 - val_loss: 1.6112 - val_accuracy: 0.2036\n",
      "Epoch 13/15\n",
      "400/400 [==============================] - 46s 114ms/step - loss: 1.6112 - accuracy: 0.1986 - val_loss: 1.6110 - val_accuracy: 0.1962\n",
      "Epoch 14/15\n",
      "400/400 [==============================] - 46s 115ms/step - loss: 1.6111 - accuracy: 0.2016 - val_loss: 1.6106 - val_accuracy: 0.1934\n",
      "Epoch 15/15\n",
      "400/400 [==============================] - 46s 114ms/step - loss: 1.6110 - accuracy: 0.1992 - val_loss: 1.6112 - val_accuracy: 0.1962\n"
     ]
    }
   ],
   "source": [
    "epoch = 15\n",
    "batch_size = 50\n",
    "\n",
    "model2.compile(\n",
    "    loss=\"categorical_crossentropy\", \n",
    "    optimizer=\"adam\", \n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "history = model2.fit(\n",
    "    x_train, \n",
    "    y_train, \n",
    "    epochs=epoch, \n",
    "    batch_size=batch_size,\n",
    "    validation_data=(x_valid, y_valid)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc5dedd",
   "metadata": {},
   "source": [
    "### Saving the trained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "073526c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"my_model0501.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6411dfa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "model2 = load_model(\"my_model0501.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ef54cef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 32, 32, 64)        1792      \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 16, 16, 128)       73856     \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 16, 16, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 8, 8, 256)         295168    \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               524416    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 1,686,981\n",
      "Trainable params: 1,686,981\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8438b272",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = model2.predict(x_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "38e211c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 0., 4., ..., 4., 1., 1.])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# å°‡outputæ”¹ç‚ºæ¨™ç±¤\n",
    "output_y = np.zeros(y_hat.shape[0])\n",
    "\n",
    "for i in range(y_hat.shape[0]):\n",
    "    for j in range(y_hat.shape[1]):\n",
    "        if y_hat[i, j] >= max(y_hat[i, ]):\n",
    "            output_y[i] = j\n",
    "\n",
    "output_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0ccc3cb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 1., 4., ..., 0., 1., 1.])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_yv = np.zeros(y_valid.shape[0])\n",
    "\n",
    "for i in range(y_valid.shape[0]):\n",
    "    for j in range(y_valid.shape[1]):\n",
    "        if y_valid[i, j] >= max(y_valid[i, ]):\n",
    "            output_yv[i] = j\n",
    "\n",
    "output_yv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5f1f4fe3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8332"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(output_y == output_yv) / len(output_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63c5a6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
