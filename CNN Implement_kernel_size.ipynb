{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5401a171",
   "metadata": {},
   "source": [
    "### Confirm Tensorflow Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "304bcb56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n",
      "2.6.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "print(tf.__version__)\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e3c573b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]='0'\n",
    "#tf.test.is_built_with_cuda()\n",
    "#tf.test.is_built_with_gpu_support()\n",
    "#tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9702e06",
   "metadata": {},
   "source": [
    "### Importing the necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9c932c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from os import listdir \n",
    "from os.path import isfile, join\n",
    "import cv2\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02686e89",
   "metadata": {},
   "source": [
    "### Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e0278dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 25000 files.\n",
      "folder: ['0', '1', '2', '3', '4']\n"
     ]
    }
   ],
   "source": [
    "#Data visiting â€“ os.walk()\n",
    "\n",
    "label_folder = []\n",
    "total_size = 0\n",
    "data_path = r\"D:\\CIFAR10_Test Image\\Training_data\"\n",
    "\n",
    "#os.walk() generates the file names(dirpath, dirnames, filenames) \n",
    "#in a directory tree by walking the tree either top-down or bottom-up.\n",
    "for root, dirts, files in os.walk(data_path): \n",
    "    for dirt in dirts:\n",
    "        label_folder.append(dirt)\n",
    "    total_size += len(files)\n",
    "\n",
    "    \n",
    "print(\"found\",total_size,\"files.\")\n",
    "print(\"folder:\",label_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ecec1475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 32, 32, 3)\n",
      "(25000,)\n"
     ]
    }
   ],
   "source": [
    "#Load image\n",
    "\n",
    "base_x_train = []\n",
    "base_y_train = []\n",
    "\n",
    "for i in range(len(label_folder)):\n",
    "    labelPath = data_path+r'\\\\'+label_folder[i]\n",
    "    \n",
    "    #listdir() returns a list containing the names of the entries in the directory given by path.\n",
    "    #isfile() is used to check whether the specified path is an existing regular file or not.\n",
    "    FileName = [f for f in listdir(labelPath) if isfile(join(labelPath, f))]\n",
    "    \n",
    "    for j in range(len(FileName)):\n",
    "        path = labelPath+r'\\\\'+FileName[j]\n",
    "        \n",
    "        #use cv2.imread read image.\n",
    "        img = cv2.imread(path,cv2.IMREAD_COLOR)\n",
    "        \n",
    "        base_x_train.append(img)\n",
    "        base_y_train.append(label_folder[i])\n",
    "\n",
    "\n",
    "print(np.array(base_x_train).shape)\n",
    "print(np.array(base_y_train).shape)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4524f916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 32, 32, 3)\n",
      "(25000, 5)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "#Convert a category vector to a binary (0 or 1) matrix-type representation\n",
    "\n",
    "base_y_train = to_categorical(base_y_train)\n",
    "\n",
    "\n",
    "print(np.array(base_x_train).shape)\n",
    "print(np.array(base_y_train).shape)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbef7560",
   "metadata": {},
   "source": [
    "### Splitting the Data into training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8b2f1af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: (20000, 32, 32, 3) (20000, 5)\n",
      "Validation data: (5000, 32, 32, 3) (5000, 5)\n"
     ]
    }
   ],
   "source": [
    "# Split into training and validation sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_valid, y_train, y_valid = train_test_split( \\\n",
    "    np.array(base_x_train), np.array(base_y_train), test_size=0.2, random_state = 0)\n",
    "\n",
    "print(\"Training data:\", x_train.shape, y_train.shape)\n",
    "print(\"Validation data:\", x_valid.shape, y_valid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845fcdd8",
   "metadata": {},
   "source": [
    "### Show the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cee274df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 0\n",
      "Answer(one-hot): [1. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "idx = random.randint(0, x_train.shape[0])\n",
    "plt.imshow(x_train[idx])\n",
    "plt.show()\n",
    "\n",
    "print(\"Answer:\", np.argmax(y_train[idx]))\n",
    "print(\"Answer(one-hot):\", y_train[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03800027",
   "metadata": {},
   "source": [
    "### Build the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "47cd5da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), padding=\"same\", activation='relu', input_shape=(32, 32, 3)),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), padding=\"same\", activation='relu'),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    tf.keras.layers.Conv2D(128, (3, 3), padding=\"same\", activation='relu'),\n",
    "    tf.keras.layers.Conv2D(128, (3, 3), padding=\"same\", activation='relu'),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    tf.keras.layers.Conv2D(256, (3, 3), padding=\"same\", activation='relu'),\n",
    "    tf.keras.layers.Conv2D(256, (3, 3), padding=\"same\", activation='relu'),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation='sigmoid'),\n",
    "    tf.keras.layers.Dense(128, activation='sigmoid'),\n",
    "    tf.keras.layers.Dense(5, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77060e81",
   "metadata": {},
   "source": [
    "### Training the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "59632ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "400/400 [==============================] - 29s 71ms/step - loss: 1.4977 - accuracy: 0.3646 - val_loss: 1.3321 - val_accuracy: 0.5066\n",
      "Epoch 2/10\n",
      "400/400 [==============================] - 29s 72ms/step - loss: 1.2526 - accuracy: 0.5139 - val_loss: 1.1381 - val_accuracy: 0.5596\n",
      "Epoch 3/10\n",
      "400/400 [==============================] - 28s 71ms/step - loss: 1.0715 - accuracy: 0.5855 - val_loss: 0.9954 - val_accuracy: 0.6226\n",
      "Epoch 4/10\n",
      "400/400 [==============================] - 29s 73ms/step - loss: 0.9519 - accuracy: 0.6346 - val_loss: 0.9046 - val_accuracy: 0.6572\n",
      "Epoch 5/10\n",
      "400/400 [==============================] - 29s 73ms/step - loss: 0.8636 - accuracy: 0.6718 - val_loss: 0.8574 - val_accuracy: 0.6718\n",
      "Epoch 6/10\n",
      "400/400 [==============================] - 29s 73ms/step - loss: 0.7916 - accuracy: 0.6981 - val_loss: 0.7948 - val_accuracy: 0.7006\n",
      "Epoch 7/10\n",
      "400/400 [==============================] - 29s 73ms/step - loss: 0.7328 - accuracy: 0.7258 - val_loss: 0.7831 - val_accuracy: 0.7066\n",
      "Epoch 8/10\n",
      "400/400 [==============================] - 29s 74ms/step - loss: 0.6704 - accuracy: 0.7497 - val_loss: 0.7244 - val_accuracy: 0.7232\n",
      "Epoch 9/10\n",
      "400/400 [==============================] - 29s 73ms/step - loss: 0.6153 - accuracy: 0.7731 - val_loss: 0.7306 - val_accuracy: 0.7184\n",
      "Epoch 10/10\n",
      "400/400 [==============================] - 28s 71ms/step - loss: 0.5585 - accuracy: 0.7980 - val_loss: 0.6581 - val_accuracy: 0.7472\n"
     ]
    }
   ],
   "source": [
    "epoch = 10\n",
    "batch_size = 50\n",
    "\n",
    "model1.compile(\n",
    "    loss=\"categorical_crossentropy\", \n",
    "    optimizer=\"sgd\", \n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "history = model1.fit(\n",
    "    x_train, \n",
    "    y_train, \n",
    "    epochs=epoch, \n",
    "    batch_size=batch_size,\n",
    "    validation_data=(x_valid, y_valid)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "47cd5da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(64, (4, 4), padding=\"same\", activation='relu', input_shape=(32, 32, 3)),\n",
    "    tf.keras.layers.Conv2D(64, (4, 4), padding=\"same\", activation='relu'),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    tf.keras.layers.Conv2D(128, (4, 4), padding=\"same\", activation='relu'),\n",
    "    tf.keras.layers.Conv2D(128, (4, 4), padding=\"same\", activation='relu'),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    tf.keras.layers.Conv2D(256, (4, 4), padding=\"same\", activation='relu'),\n",
    "    tf.keras.layers.Conv2D(256, (4, 4), padding=\"same\", activation='relu'),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation='sigmoid'),\n",
    "    tf.keras.layers.Dense(128, activation='sigmoid'),\n",
    "    tf.keras.layers.Dense(5, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "59632ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "400/400 [==============================] - 93s 211ms/step - loss: 1.4374 - accuracy: 0.4042 - val_loss: 1.2896 - val_accuracy: 0.4792\n",
      "Epoch 2/10\n",
      "400/400 [==============================] - 83s 207ms/step - loss: 1.1736 - accuracy: 0.5450 - val_loss: 1.0719 - val_accuracy: 0.5824\n",
      "Epoch 3/10\n",
      "400/400 [==============================] - 82s 205ms/step - loss: 1.0244 - accuracy: 0.6060 - val_loss: 0.9701 - val_accuracy: 0.6260\n",
      "Epoch 4/10\n",
      "400/400 [==============================] - 82s 205ms/step - loss: 0.9066 - accuracy: 0.6556 - val_loss: 0.9096 - val_accuracy: 0.6428\n",
      "Epoch 5/10\n",
      "400/400 [==============================] - 82s 205ms/step - loss: 0.8236 - accuracy: 0.6895 - val_loss: 0.8691 - val_accuracy: 0.6628: 0.8328 - ETA: 24s - loss: 0.8306 - accuracy - E\n",
      "Epoch 6/10\n",
      "400/400 [==============================] - 83s 207ms/step - loss: 0.7524 - accuracy: 0.7157 - val_loss: 0.8330 - val_accuracy: 0.6730\n",
      "Epoch 7/10\n",
      "400/400 [==============================] - 85s 211ms/step - loss: 0.6891 - accuracy: 0.7433 - val_loss: 0.7205 - val_accuracy: 0.7314\n",
      "Epoch 8/10\n",
      "400/400 [==============================] - 87s 218ms/step - loss: 0.6235 - accuracy: 0.7704 - val_loss: 0.7072 - val_accuracy: 0.7286\n",
      "Epoch 9/10\n",
      "400/400 [==============================] - 85s 212ms/step - loss: 0.5604 - accuracy: 0.7954 - val_loss: 0.6323 - val_accuracy: 0.7654\n",
      "Epoch 10/10\n",
      "400/400 [==============================] - 84s 210ms/step - loss: 0.5059 - accuracy: 0.8205 - val_loss: 0.6238 - val_accuracy: 0.7646\n"
     ]
    }
   ],
   "source": [
    "epoch = 10\n",
    "batch_size = 50\n",
    "\n",
    "model2.compile(\n",
    "    loss=\"categorical_crossentropy\", \n",
    "    optimizer=\"sgd\", \n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "history = model2.fit(\n",
    "    x_train, \n",
    "    y_train, \n",
    "    epochs=epoch, \n",
    "    batch_size=batch_size,\n",
    "    validation_data=(x_valid, y_valid)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "47cd5da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), padding=\"same\", activation='relu', input_shape=(32, 32, 3)),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), padding=\"same\", activation='relu'),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    tf.keras.layers.Conv2D(128, (3, 3), padding=\"same\", activation='relu'),\n",
    "    tf.keras.layers.Conv2D(128, (3, 3), padding=\"same\", activation='relu'),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    tf.keras.layers.Conv2D(256, (4, 4), padding=\"same\", activation='relu'),\n",
    "    tf.keras.layers.Conv2D(256, (4, 4), padding=\"same\", activation='relu'),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation='sigmoid'),\n",
    "    tf.keras.layers.Dense(128, activation='sigmoid'),\n",
    "    tf.keras.layers.Dense(5, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "59632ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "400/400 [==============================] - 52s 125ms/step - loss: 1.4487 - accuracy: 0.3987 - val_loss: 1.2771 - val_accuracy: 0.5170\n",
      "Epoch 2/10\n",
      "400/400 [==============================] - 50s 126ms/step - loss: 1.1721 - accuracy: 0.5451 - val_loss: 1.1595 - val_accuracy: 0.5516\n",
      "Epoch 3/10\n",
      "400/400 [==============================] - 50s 124ms/step - loss: 1.0076 - accuracy: 0.6151 - val_loss: 0.9633 - val_accuracy: 0.6346\n",
      "Epoch 4/10\n",
      "400/400 [==============================] - 50s 124ms/step - loss: 0.8845 - accuracy: 0.6651 - val_loss: 0.8780 - val_accuracy: 0.6652\n",
      "Epoch 5/10\n",
      "400/400 [==============================] - 49s 123ms/step - loss: 0.7957 - accuracy: 0.6999 - val_loss: 0.7790 - val_accuracy: 0.7020\n",
      "Epoch 6/10\n",
      "400/400 [==============================] - 49s 122ms/step - loss: 0.7199 - accuracy: 0.7338 - val_loss: 0.7372 - val_accuracy: 0.7270\n",
      "Epoch 7/10\n",
      "400/400 [==============================] - 49s 123ms/step - loss: 0.6472 - accuracy: 0.7660 - val_loss: 0.7053 - val_accuracy: 0.7310\n",
      "Epoch 8/10\n",
      "400/400 [==============================] - 50s 124ms/step - loss: 0.5791 - accuracy: 0.7907 - val_loss: 0.6835 - val_accuracy: 0.7422\n",
      "Epoch 9/10\n",
      "400/400 [==============================] - 50s 124ms/step - loss: 0.5204 - accuracy: 0.8138 - val_loss: 0.6189 - val_accuracy: 0.7730\n",
      "Epoch 10/10\n",
      "400/400 [==============================] - 49s 123ms/step - loss: 0.4562 - accuracy: 0.8434 - val_loss: 0.6078 - val_accuracy: 0.7716\n"
     ]
    }
   ],
   "source": [
    "epoch = 10\n",
    "batch_size = 50\n",
    "\n",
    "model3.compile(\n",
    "    loss=\"categorical_crossentropy\", \n",
    "    optimizer=\"sgd\", \n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "history = model3.fit(\n",
    "    x_train, \n",
    "    y_train, \n",
    "    epochs=epoch, \n",
    "    batch_size=batch_size,\n",
    "    validation_data=(x_valid, y_valid)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc5dedd",
   "metadata": {},
   "source": [
    "### Saving the trained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "073526c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"my_model0501.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6411dfa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "model2 = load_model(\"my_model0501.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ef54cef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 32, 32, 64)        1792      \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 16, 16, 128)       73856     \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 16, 16, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 8, 8, 256)         295168    \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               524416    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 1,686,981\n",
      "Trainable params: 1,686,981\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8438b272",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = model2.predict(x_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "38e211c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 0., 4., ..., 4., 1., 1.])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# å°‡outputæ”¹ç‚ºæ¨™ç±¤\n",
    "output_y = np.zeros(y_hat.shape[0])\n",
    "\n",
    "for i in range(y_hat.shape[0]):\n",
    "    for j in range(y_hat.shape[1]):\n",
    "        if y_hat[i, j] >= max(y_hat[i, ]):\n",
    "            output_y[i] = j\n",
    "\n",
    "output_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0ccc3cb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 1., 4., ..., 0., 1., 1.])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_yv = np.zeros(y_valid.shape[0])\n",
    "\n",
    "for i in range(y_valid.shape[0]):\n",
    "    for j in range(y_valid.shape[1]):\n",
    "        if y_valid[i, j] >= max(y_valid[i, ]):\n",
    "            output_yv[i] = j\n",
    "\n",
    "output_yv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5f1f4fe3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8332"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(output_y == output_yv) / len(output_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63c5a6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
